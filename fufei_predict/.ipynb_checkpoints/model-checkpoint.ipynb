{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from redshiftpool import RedShift\n",
    "from mysqlpool import Mysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create connect redshift\n",
      "create connect\n"
     ]
    }
   ],
   "source": [
    "redshift = RedShift()\n",
    "mysql = Mysql()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql_train_neg = \"select c.token,to_char(client_time,'YYYY-MM-DD HH24:MI:SS'),c.sta_key,to_char(create_time,'YYYY-MM-DD HH24:MI:SS') \\\n",
    "from (select b.* from (select token from sta_new_user where server_time>='2017-06-01' \\\n",
    "and server_time<'2017-06-06' and product_id=600027) a ,(select * from (select token,client_time,sta_key,ROW_NUMBER () OVER (PARTITION BY token order by client_time asc) as row \\\n",
    "from sta_event_game_publish where server_time>='2017-06-01' and server_time<'2017-06-11' and product_id=600027 \\\n",
    "and substring(sta_key,1,1)='T') t where row<=30) b where a.token=b.token) c left join (select token,min(create_time) as create_time \\\n",
    "from game_iap where create_time>='2017-06-01' and product_id=600027 group by token)d on c.token=d.token where d.token is null;\"\n",
    "\n",
    "sql_train_neg = \"select c.token,to_char(client_time,'YYYY-MM-DD HH24:MI:SS'),c.sta_key,to_char(create_time,'YYYY-MM-DD HH24:MI:SS') \\\n",
    "from (select b.* from (select token from sta_new_user where server_time>='2017-07-01' \\\n",
    "and server_time<'2017-07-06' and product_id=600027) a ,(select * from (select token,client_time,sta_key,ROW_NUMBER () OVER (PARTITION BY token order by client_time asc) as row \\\n",
    "from sta_event_game_publish where server_time>='2017-07-01' and server_time<'2017-07-11' and product_id=600027 \\\n",
    "and substring(sta_key,1,1)='T') t where row<=30) b where a.token=b.token) c left join (select token,min(create_time) as create_time \\\n",
    "from game_iap where create_time>='2017-07-01' and product_id=600027 group by token)d on c.token=d.token where d.token is null;\"\n",
    "\n",
    "\n",
    "sql_train_pos = \"select c.token,to_char(client_time,'YYYY-MM-DD HH24:MI:SS'),c.sta_key,to_char(create_time,'YYYY-MM-DD HH24:MI:SS') \\\n",
    "from (select b.* from (select token from sta_new_user where server_time>='2017-03-01' and product_id=600027) a ,\\\n",
    "(select * from (select token,client_time,sta_key,ROW_NUMBER () OVER (PARTITION BY token order by client_time asc) as row \\\n",
    "from sta_event_game_publish where server_time>='2017-03-01' and product_id=600027 \\\n",
    "and substring(sta_key,1,1)='T') t where row<=30) b where a.token=b.token) c join (select token,min(create_time) as create_time \\\n",
    "from game_iap where create_time>='2017-04-01' and product_id=600027 group by token)d on c.token=d.token;\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getRawData(s,filename):\n",
    "    raw = redshift.getAll(s)\n",
    "    fileObject = open(filename, 'w')  \n",
    "    for r in raw:  \n",
    "        fileObject.write(str(r))  \n",
    "        fileObject.write('\\n')  \n",
    "    fileObject.close() \n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#raw_train_neg = getRawData(sql_train_neg,'raw_train_neg.txt')\n",
    "raw_train_pos = getRawData(sql_train_pos,'raw_train_pos.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_train_neg = getRawData(sql_train_neg,'raw_train_neg.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#公共方法\n",
    "def getDic(data):\n",
    "    positiveDataDic={} #是一个储存正样本的字典\n",
    "    negativeDataDic={} #是一个储存负样本的字典\n",
    "    for i in range(len(data)):\n",
    "        key=data[i][0]\n",
    "        value = []\n",
    "        value.extend(data[i][1:3])\n",
    "        if data[i][3]:\n",
    "            if positiveDataDic.has_key(key):\n",
    "                positiveDataDic[key].extend([value])\n",
    "            else:\n",
    "                positiveDataDic[key]=[value]\n",
    "        else:\n",
    "            if negativeDataDic.has_key(key):\n",
    "                negativeDataDic[key].extend([value])\n",
    "            else:\n",
    "                negativeDataDic[key]=[value]\n",
    "\n",
    "    return positiveDataDic,negativeDataDic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ffc06075dd474f3282a757bdaa5fa771', '2017-07-04 06:05:38', 'T0S', None), ('ffc06075dd474f3282a757bdaa5fa771', '2017-07-04 06:05:38', 'T0K', None), ('ffc06075dd474f3282a757bdaa5fa771', '2017-07-04 06:05:38', 'T08', None), ('ffc06075dd474f3282a757bdaa5fa771', '2017-07-04 06:05:57', 'T07', None), ('ffc06075dd474f3282a757bdaa5fa771', '2017-07-04 06:06:11', 'T09', None), ('ffc06075dd474f3282a757bdaa5fa771', '2017-07-04 06:06:13', 'T09', None), ('ffc06075dd474f3282a757bdaa5fa771', '2017-07-04 06:07:02', 'T08', None), ('ffc06075dd474f3282a757bdaa5fa771', '2017-07-04 06:07:02', 'T0K', None), ('ffc06075dd474f3282a757bdaa5fa771', '2017-07-04 06:07:02', 'T0S', None), ('ffc06075dd474f3282a757bdaa5fa771', '2017-07-04 06:07:20', 'T07', None)]\n"
     ]
    }
   ],
   "source": [
    "print raw_train_neg[-10:]\n",
    "dic_train_neg_pos,dic_train_neg_neg = getDic(raw_train_neg)\n",
    "dic_train_pos_pos,dic_train_pos_neg = getDic(raw_train_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keys=['GIB','T01','T02','T03','T04','T05','T06','T07','T08','T09','T0F','T0G','T0H','T0I','T0J','T0K','T0L','T0M','T0O','T0P','T0R','T0S','T0T','T0U','T0V','T0N']\n",
    "\n",
    "key_indices = dict((key, i) for i, key in enumerate(keys))\n",
    "indices_key = dict((i, key) for i, key in enumerate(keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def makeTrain(dic):\n",
    "    result = []\n",
    "    for (k,v) in dic.items():\n",
    "        if len(v)<21:\n",
    "            continue\n",
    "        else:\n",
    "            v.sort()\n",
    "            value = [key_indices[v[i][1]] for i in range(1,21)]\n",
    "            result.append(value)\n",
    "    \n",
    "    return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7376\n",
      "12184\n",
      "[[21, 1, 4, 5, 9, 9, 15, 21, 8, 23, 7, 9, 9, 8, 15, 21, 7, 9, 9, 21], [21, 1, 4, 5, 9, 9, 21, 8, 15, 2, 13, 23, 23, 7, 9, 9, 8, 15, 21, 7], [21, 1, 4, 5, 9, 9, 8, 15, 21, 23, 7, 9, 9, 8, 15, 21, 7, 9, 9, 8]]\n",
      "[[21, 1, 2, 13, 23, 4, 2, 13, 16, 1, 2, 13, 23, 2, 13, 23, 5, 16, 1, 16], [21, 1, 2, 4, 13, 23, 5, 9, 9, 15, 21, 8, 2, 13, 23, 23, 7, 9, 9, 8], [2, 13, 21, 23, 1, 4, 5, 9, 9, 15, 21, 8, 23, 10, 7, 9, 9, 8, 15, 21]]\n"
     ]
    }
   ],
   "source": [
    "pos = makeTrain(dic_train_pos_pos)\n",
    "neg = makeTrain(dic_train_neg_neg)\n",
    "\n",
    "print len(pos)\n",
    "print len(neg)\n",
    "\n",
    "print pos[0:3]\n",
    "print neg[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "876\n",
      "876 876\n",
      "13000 13000\n",
      "1752 1752\n",
      "(1752,)\n",
      "[ 1.  1.  1. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import copy\n",
    "\n",
    "random.shuffle(neg)\n",
    "\n",
    "tlen = 6500\n",
    "\n",
    "trainX = copy.deepcopy(pos[0:tlen])\n",
    "trainX.extend(neg[0:tlen])\n",
    "trainY = np.ones(tlen)\n",
    "trainY = np.append(trainY, np.zeros(tlen))\n",
    "\n",
    "testX = copy.deepcopy(pos[tlen:])\n",
    "leng = len(testX)\n",
    "testY = np.ones(leng)\n",
    "\n",
    "print leng\n",
    "print len(testX),len(testY)\n",
    "testX.extend(neg[tlen:tlen+leng])\n",
    "testY = np.append(testY, np.zeros(leng))\n",
    "\n",
    "print len(trainX),len(trainY)\n",
    "print len(testX),len(testY)\n",
    "print testY.shape\n",
    "\n",
    "print trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainX = np.array(trainX)\n",
    "testX = np.array(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 128)         3328      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 128)         131584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, None, 128)         512       \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 275,841.0\n",
      "Trainable params: 275,329\n",
      "Non-trainable params: 512.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(26, 128))\n",
    "model.add(LSTM(128, dropout=0.3, recurrent_dropout=0.2, return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(128, dropout=0.3, recurrent_dropout=0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(64,activation='tanh'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 13000 samples, validate on 1752 samples\n",
      "Epoch 1/20\n",
      "13000/13000 [==============================] - 75s - loss: 0.4453 - acc: 0.7751 - val_loss: 0.4529 - val_acc: 0.7831\n",
      "Epoch 2/20\n",
      "13000/13000 [==============================] - 70s - loss: 0.4037 - acc: 0.7864 - val_loss: 0.3646 - val_acc: 0.8088\n",
      "Epoch 3/20\n",
      "13000/13000 [==============================] - 68s - loss: 0.3943 - acc: 0.7909 - val_loss: 0.3681 - val_acc: 0.7979\n",
      "Epoch 4/20\n",
      "13000/13000 [==============================] - 68s - loss: 0.3896 - acc: 0.7924 - val_loss: 0.3600 - val_acc: 0.8082\n",
      "Epoch 5/20\n",
      "13000/13000 [==============================] - 70s - loss: 0.3778 - acc: 0.8023 - val_loss: 0.3629 - val_acc: 0.8139\n",
      "Epoch 6/20\n",
      "13000/13000 [==============================] - 70s - loss: 0.3776 - acc: 0.8015 - val_loss: 0.3620 - val_acc: 0.8225\n",
      "Epoch 7/20\n",
      "13000/13000 [==============================] - 68s - loss: 0.3732 - acc: 0.8092 - val_loss: 0.3539 - val_acc: 0.8219\n",
      "Epoch 8/20\n",
      "13000/13000 [==============================] - 68s - loss: 0.3728 - acc: 0.8077 - val_loss: 0.3503 - val_acc: 0.8208\n",
      "Epoch 9/20\n",
      "13000/13000 [==============================] - 71s - loss: 0.3721 - acc: 0.8086 - val_loss: 0.3517 - val_acc: 0.8288\n",
      "Epoch 10/20\n",
      "13000/13000 [==============================] - 73s - loss: 0.3647 - acc: 0.8111 - val_loss: 0.3499 - val_acc: 0.8276\n",
      "Epoch 11/20\n",
      "13000/13000 [==============================] - 71s - loss: 0.3649 - acc: 0.8102 - val_loss: 0.3486 - val_acc: 0.8288\n",
      "Epoch 12/20\n",
      "13000/13000 [==============================] - 69s - loss: 0.3641 - acc: 0.8118 - val_loss: 0.3486 - val_acc: 0.8208\n",
      "Epoch 13/20\n",
      "13000/13000 [==============================] - 69s - loss: 0.3624 - acc: 0.8155 - val_loss: 0.3742 - val_acc: 0.8065\n",
      "Epoch 14/20\n",
      "13000/13000 [==============================] - 73s - loss: 0.3651 - acc: 0.8136 - val_loss: 0.3432 - val_acc: 0.8276\n",
      "Epoch 15/20\n",
      "13000/13000 [==============================] - 68s - loss: 0.3597 - acc: 0.8158 - val_loss: 0.3495 - val_acc: 0.8236\n",
      "Epoch 16/20\n",
      "13000/13000 [==============================] - 73s - loss: 0.3584 - acc: 0.8176 - val_loss: 0.3406 - val_acc: 0.8305\n",
      "Epoch 17/20\n",
      "13000/13000 [==============================] - 70s - loss: 0.3554 - acc: 0.8181 - val_loss: 0.3391 - val_acc: 0.8305\n",
      "Epoch 18/20\n",
      "13000/13000 [==============================] - 74s - loss: 0.3543 - acc: 0.8205 - val_loss: 0.3425 - val_acc: 0.8242\n",
      "Epoch 19/20\n",
      "13000/13000 [==============================] - 69s - loss: 0.3519 - acc: 0.8192 - val_loss: 0.3393 - val_acc: 0.8299\n",
      "Epoch 20/20\n",
      "13000/13000 [==============================] - 70s - loss: 0.3500 - acc: 0.8209 - val_loss: 0.3415 - val_acc: 0.8311\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x133ac3ed0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(trainX, trainY,\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "          shuffle=True,\n",
    "          validation_data=(testX, testY))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1499668437.66\n",
      "1499668456.61\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "print time.time()\n",
    "result = model.predict(trainX)\n",
    "print time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13000\n",
      "5903\n",
      "1890\n",
      "5145\n",
      "850\n"
     ]
    }
   ],
   "source": [
    "#验证训练数据\n",
    "print len(result)\n",
    "posl = result[:tlen]\n",
    "negl = result[tlen:]\n",
    "\n",
    "posloc = np.where(posl>=0.35)\n",
    "negloc = np.where(negl>=0.35)\n",
    "print len(posloc[0])\n",
    "print len(negloc[0])\n",
    "\n",
    "\n",
    "posloc = np.where(posl>=0.45)\n",
    "negloc = np.where(negl>=0.45)\n",
    "\n",
    "print len(posloc[0])\n",
    "print len(negloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.42310295]\n",
      " [ 0.42854559]\n",
      " [ 0.42056403]\n",
      " ..., \n",
      " [ 0.0621783 ]\n",
      " [ 0.0718566 ]\n",
      " [ 0.07768767]]\n"
     ]
    }
   ],
   "source": [
    "print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultpos = model.predict(np.array(pos))\n",
    "resultneg = model.predict(np.array(neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7376\n",
      "6978\n",
      "分到正类百分比： 0.946041214751\n",
      "12184\n",
      "4926\n",
      "分到负类百分比： 0.595699277741\n"
     ]
    }
   ],
   "source": [
    "rate=0.2\n",
    "\n",
    "posloc = np.where(resultpos>=rate)\n",
    "negloc = np.where(resultpos<=rate)\n",
    "print len(resultpos)\n",
    "print len(posloc[0])\n",
    "print \"分到正类百分比：\",len(posloc[0])*1.0/len(resultpos)\n",
    "\n",
    "posloc = np.where(resultneg>=rate)\n",
    "negloc = np.where(resultneg<=rate)\n",
    "print len(resultneg)\n",
    "print len(posloc[0])\n",
    "print \"分到负类百分比：\",len(negloc[0])*1.0/len(resultneg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 20)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)          (None, 20, 32)        800                                          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                    (None, 20, 128)       82432                                        \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                    (None, 32)            20608                                        \n",
      "____________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)          (None, 1, 32)         800                                          \n",
      "____________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)              (None, 32)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 32)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 64)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 64)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 512)           33280                                        \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 512)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 64)            32832                                        \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 64)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 1)             65                                           \n",
      "====================================================================================================\n",
      "Total params: 170,817.0\n",
      "Trainable params: 170,817.0\n",
      "Non-trainable params: 0.0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#推荐使用的模型\n",
    "from keras.layers import Input, Embedding, LSTM, Dense,Flatten, Dropout, merge\n",
    "from keras.layers import Input, Activation, Permute, Dropout, add, dot, concatenate\n",
    "from keras.layers.core import Reshape\n",
    "from keras.models import Model\n",
    "input_1=Input(shape=(20,), dtype='int32')\n",
    "input_2=Input(shape=(1,), dtype='int32')\n",
    "x1=Embedding(output_dim=32, input_dim=25, input_length=20)(input_1)\n",
    "x1=LSTM(128, return_sequences=True)(x1)\n",
    "x1=LSTM(32)(x1)\n",
    "x1=Reshape((-1,))(x1)\n",
    "x2=Embedding(output_dim=32, input_dim=25, input_length=1)(input_2)\n",
    "x2=Flatten()(x2)\n",
    "x = concatenate([x1, x2])\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "out = Dense(1, activation='sigmoid')(x)\n",
    "model = Model([input_1, input_2], out)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1. ...,  0.  0.  0.]\n",
      "[ 1.  1.  1. ...,  0.  0.  0.]\n",
      "\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "⋮ \n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.LongTensor of size 13000]\n",
      "\n",
      "Variable containing:\n",
      "   21     1     2  ...     23     9     2\n",
      "   21     1     5  ...     23     3     7\n",
      "   21     1     2  ...     13    23     8\n",
      "       ...          ⋱          ...       \n",
      "    1    21     4  ...      9     9    21\n",
      "   21     1     2  ...      9     8    15\n",
      "   21     1     4  ...      9     9     8\n",
      "[torch.LongTensor of size 1752x20]\n",
      "\n",
      "[ 1.  1.  1. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "#pytorch\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "from torch import nn\n",
    "torch.manual_seed(1)    # reproducible\n",
    "\n",
    "BATCH_SIZE = 32      # 批训练的数据个数\n",
    "\n",
    "\n",
    "torchx = torch.from_numpy(trainX)\n",
    "#torchx = Variable(torchx)\n",
    "print trainY\n",
    "#trainY.dtype = 'int64'\n",
    "torchy = torch.from_numpy(trainY).type(torch.LongTensor)\n",
    "print trainY\n",
    "print torchy\n",
    "torch_test_x = Variable(torch.from_numpy(testX), volatile=True) \n",
    "print torch_test_x\n",
    "torch_test_y = testY.squeeze()    # covert to numpy array\n",
    "print torch_test_y\n",
    "\n",
    "#torchy = Variable(torchy)\n",
    "\n",
    "# 先转换成 torch 能识别的 Dataset\n",
    "torch_dataset = Data.TensorDataset(data_tensor=torchx, target_tensor=torchy)\n",
    "\n",
    "# 把 dataset 放入 DataLoader\n",
    "loader = Data.DataLoader(\n",
    "    dataset=torch_dataset,      # torch TensorDataset format\n",
    "    batch_size=BATCH_SIZE,      # mini batch size\n",
    "    shuffle=True,               # 要不要打乱数据 (打乱比较好)\n",
    "    num_workers=2,              # 多线程来读数据\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net (\n",
      "  (embed): Embedding(26, 128)\n",
      "  (rnn): LSTM(128, 128, batch_first=True)\n",
      "  (hidden): Linear (128 -> 64)\n",
      "  (predict): Linear (64 -> 2)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(torch.nn.Module):  # 继承 torch 的 Module\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()     # 继承 __init__ 功能\n",
    "        # 定义每层用什么样的形式\n",
    "        self.embed = torch.nn.Embedding(26, 128)\n",
    "        self.rnn = nn.LSTM(     # LSTM 效果要比 nn.RNN() 好多了\n",
    "            input_size=128,      # \n",
    "            hidden_size=128,     # rnn hidden unit\n",
    "            num_layers=2,       # 有几层 RNN layers\n",
    "            batch_first=True,   # input & output 会是以 batch size 为第一维度的特征集 e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "        self.hidden = torch.nn.Linear(128, 64)   # 隐藏层线性输出\n",
    "        self.predict = torch.nn.Linear(64, 2)   # 输出层线性输出\n",
    "\n",
    "    def forward(self, x):   # 这同时也是 Module 中的 forward 功能\n",
    "        x = self.embed(x)\n",
    "        r_out, (h_n, h_c) = self.rnn(x, None) \n",
    "        #x = F.relu(self.hidden(x))      # 激励函数(隐藏层的线性值)\n",
    "        x = F.relu(self.hidden(r_out[:, -1, :]))\n",
    "        x = self.predict(x)\n",
    "        return x\n",
    "    \n",
    "rnn = Net()\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# optimizer 是训练的工具\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=0.5)  # 传入 net 的所有参数, 学习率\n",
    "#optimizer = torch.optim.Adam(rnn.parameters(), lr=0.1) \n",
    "\n",
    "# 算误差的时候, 注意真实值!不是! one-hot 形式的, 而是1D Tensor, (batch,)\n",
    "# 但是预测值是2D tensor (batch, n_classes)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for t in range(1):\n",
    "    for step, (batch_x, batch_y) in enumerate(loader):\n",
    "        pass\n",
    "        #print type(batch_x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ..., 1 0 0]\n",
      "('Epoch: ', 0, '| train loss: 0.8835', '| test accuracy: 0.81')\n",
      "[0 1 1 ..., 0 0 0]\n",
      "('Epoch: ', 1, '| train loss: 0.3060', '| test accuracy: 0.79')\n",
      "[0 1 1 ..., 0 0 0]\n",
      "('Epoch: ', 2, '| train loss: 0.2576', '| test accuracy: 0.80')\n",
      "[0 1 1 ..., 1 0 1]\n",
      "('Epoch: ', 3, '| train loss: 0.2747', '| test accuracy: 0.79')\n",
      "[0 1 1 ..., 0 0 0]\n",
      "('Epoch: ', 4, '| train loss: 0.3129', '| test accuracy: 0.82')\n",
      "[0 1 1 ..., 0 0 0]\n",
      "('Epoch: ', 5, '| train loss: 0.5796', '| test accuracy: 0.72')\n",
      "[0 1 1 ..., 0 0 0]\n",
      "('Epoch: ', 6, '| train loss: 0.2338', '| test accuracy: 0.81')\n",
      "[0 1 1 ..., 0 0 0]\n",
      "('Epoch: ', 7, '| train loss: 0.2942', '| test accuracy: 0.80')\n",
      "[0 1 1 ..., 0 0 1]\n",
      "('Epoch: ', 8, '| train loss: 0.1359', '| test accuracy: 0.80')\n",
      "[0 1 1 ..., 0 0 0]\n",
      "('Epoch: ', 9, '| train loss: 0.3328', '| test accuracy: 0.83')\n"
     ]
    }
   ],
   "source": [
    "for t in range(10):\n",
    "    for step, (batch_x, batch_y) in enumerate(loader):\n",
    "        b_x = Variable(batch_x)              # reshape x to (batch, time_step, input_size)\n",
    "        b_y = Variable(batch_y)                               # batch y\n",
    "        #print b_x\n",
    "        \n",
    "        output = rnn(b_x)         # rnn output\n",
    "        loss = loss_func(output, b_y)                   # cross entropy loss\n",
    "        optimizer.zero_grad()                           # clear gradients for this training step\n",
    "        loss.backward()                                 # backpropagation, compute gradients\n",
    "        optimizer.step() \n",
    "        \n",
    "        \n",
    "    test_output = rnn(torch_test_x)\n",
    "    #test_output = rnn(b_x)\n",
    "    \n",
    "    #print test_output\n",
    "    pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "    print pred_y\n",
    "    accuracy = sum(pred_y == torch_test_y) / float(torch_test_y.size)\n",
    "    print('Epoch: ', t, '| train loss: %.4f' % loss.data[0], '| test accuracy: %.2f' % accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_output = rnn(b_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 6.8061 -6.6220\n",
      " 7.2712 -6.5124\n",
      " 6.8020 -6.4809\n",
      " 6.8020 -6.4809\n",
      " 6.8489 -6.5292\n",
      " 6.6334 -6.6031\n",
      " 6.8020 -6.4809\n",
      " 7.1402 -6.7817\n",
      "[torch.FloatTensor of size 8x2]\n",
      "\n",
      "Variable containing:\n",
      " 0.0000e+00\n",
      " 4.6072e+18\n",
      " 0.0000e+00\n",
      " 0.0000e+00\n",
      " 4.6072e+18\n",
      " 4.6072e+18\n",
      " 0.0000e+00\n",
      " 0.0000e+00\n",
      "[torch.LongTensor of size 8]\n",
      "\n",
      "\n",
      " 0.0000e+00\n",
      " 4.6072e+18\n",
      " 0.0000e+00\n",
      " 0.0000e+00\n",
      " 4.6072e+18\n",
      " 4.6072e+18\n",
      " 0.0000e+00\n",
      " 0.0000e+00\n",
      "[torch.LongTensor of size 8]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print test_output\n",
    "print b_y\n",
    "print batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
