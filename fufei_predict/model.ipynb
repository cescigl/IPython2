{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from redshiftpool import RedShift\n",
    "from mysqlpool import Mysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create connect redshift\n",
      "create connect\n"
     ]
    }
   ],
   "source": [
    "redshift = RedShift()\n",
    "#mysql = Mysql()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql_train_neg = \"select c.token,to_char(client_time,'YYYY-MM-DD HH24:MI:SS'),c.sta_key,to_char(create_time,'YYYY-MM-DD HH24:MI:SS') \\\n",
    "from (select b.* from (select token from sta_new_user where server_time>='2017-06-01' \\\n",
    "and server_time<'2017-06-06' and product_id=600027) a ,(select * from (select token,client_time,sta_key,ROW_NUMBER () OVER (PARTITION BY token order by client_time asc) as row \\\n",
    "from sta_event_game_publish where server_time>='2017-06-01' and server_time<'2017-06-11' and product_id=600027 \\\n",
    "and substring(sta_key,1,1)='T') t where row<=30) b where a.token=b.token) c left join (select token,min(create_time) as create_time \\\n",
    "from game_iap where create_time>='2017-06-01' and product_id=600027 group by token)d on c.token=d.token where d.token is null;\"\n",
    "\n",
    "sql_train_neg = \"select c.token,to_char(client_time,'YYYY-MM-DD HH24:MI:SS'),c.sta_key,to_char(create_time,'YYYY-MM-DD HH24:MI:SS') \\\n",
    "from (select b.* from (select token from sta_new_user where server_time>='2017-07-01' \\\n",
    "and server_time<'2017-07-06' and product_id=600027) a ,(select * from (select token,client_time,sta_key,ROW_NUMBER () OVER (PARTITION BY token order by client_time asc) as row \\\n",
    "from sta_event_game_publish where server_time>='2017-07-01' and server_time<'2017-07-11' and product_id=600027 \\\n",
    "and substring(sta_key,1,1)='T') t where row<=30) b where a.token=b.token) c left join (select token,min(create_time) as create_time \\\n",
    "from game_iap where create_time>='2017-07-01' and product_id=600027 group by token)d on c.token=d.token where d.token is null;\"\n",
    "\n",
    "\n",
    "sql_train_pos = \"select c.token,to_char(client_time,'YYYY-MM-DD HH24:MI:SS'),c.sta_key,to_char(create_time,'YYYY-MM-DD HH24:MI:SS') \\\n",
    "from (select b.* from (select token from sta_new_user where server_time>='2017-03-01' and product_id=600027) a ,\\\n",
    "(select * from (select token,client_time,sta_key,ROW_NUMBER () OVER (PARTITION BY token order by client_time asc) as row \\\n",
    "from sta_event_game_publish where server_time>='2017-03-01' and product_id=600027 \\\n",
    "and substring(sta_key,1,1)='T') t where row<=30) b where a.token=b.token) c join (select token,min(create_time) as create_time \\\n",
    "from game_iap where create_time>='2017-04-01' and product_id=600027 group by token)d on c.token=d.token;\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getRawData(s,filename):\n",
    "    raw = redshift.getAll(s)\n",
    "    fileObject = open(filename, 'w')  \n",
    "    for r in raw:  \n",
    "        fileObject.write(str(r))  \n",
    "        fileObject.write('\\n')  \n",
    "    fileObject.close() \n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#raw_train_neg = getRawData(sql_train_neg,'raw_train_neg.txt')\n",
    "raw_train_pos = getRawData(sql_train_pos,'raw_train_pos.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_train_neg = getRawData(sql_train_neg,'raw_train_neg.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#公共方法\n",
    "def getDic(data):\n",
    "    positiveDataDic={} #是一个储存正样本的字典\n",
    "    negativeDataDic={} #是一个储存负样本的字典\n",
    "    for i in range(len(data)):\n",
    "        key=data[i][0]\n",
    "        value = []\n",
    "        value.extend(data[i][1:3])\n",
    "        if data[i][3]:\n",
    "            if positiveDataDic.has_key(key):\n",
    "                positiveDataDic[key].extend([value])\n",
    "            else:\n",
    "                positiveDataDic[key]=[value]\n",
    "        else:\n",
    "            if negativeDataDic.has_key(key):\n",
    "                negativeDataDic[key].extend([value])\n",
    "            else:\n",
    "                negativeDataDic[key]=[value]\n",
    "\n",
    "    return positiveDataDic,negativeDataDic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ffc06075dd474f3282a757bdaa5fa771', '2017-07-04 06:05:38', 'T0S', None), ('ffc06075dd474f3282a757bdaa5fa771', '2017-07-04 06:05:38', 'T08', None), ('ffc06075dd474f3282a757bdaa5fa771', '2017-07-04 06:05:38', 'T0K', None), ('ffc06075dd474f3282a757bdaa5fa771', '2017-07-04 06:05:57', 'T07', None), ('ffc06075dd474f3282a757bdaa5fa771', '2017-07-04 06:06:11', 'T09', None), ('ffc06075dd474f3282a757bdaa5fa771', '2017-07-04 06:06:13', 'T09', None), ('ffc06075dd474f3282a757bdaa5fa771', '2017-07-04 06:07:02', 'T08', None), ('ffc06075dd474f3282a757bdaa5fa771', '2017-07-04 06:07:02', 'T0S', None), ('ffc06075dd474f3282a757bdaa5fa771', '2017-07-04 06:07:02', 'T0K', None), ('ffc06075dd474f3282a757bdaa5fa771', '2017-07-04 06:07:20', 'T07', None)]\n"
     ]
    }
   ],
   "source": [
    "print raw_train_neg[-10:]\n",
    "dic_train_neg_pos,dic_train_neg_neg = getDic(raw_train_neg)\n",
    "dic_train_pos_pos,dic_train_pos_neg = getDic(raw_train_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keys=['GIB','T01','T02','T03','T04','T05','T06','T07','T08','T09','T0F','T0G','T0H','T0I','T0J','T0K','T0L','T0M','T0O','T0P','T0R','T0S','T0T','T0U','T0V','T0N']\n",
    "\n",
    "key_indices = dict((key, i) for i, key in enumerate(keys))\n",
    "indices_key = dict((i, key) for i, key in enumerate(keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def makeTrain(dic):\n",
    "    result = []\n",
    "    for (k,v) in dic.items():\n",
    "        if len(v)<21:\n",
    "            continue\n",
    "        else:\n",
    "            v.sort()\n",
    "            value = [key_indices[v[i][1]] for i in range(1,21)]\n",
    "            result.append(value)\n",
    "    \n",
    "    return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7515\n",
      "12186\n",
      "[[21, 1, 4, 5, 9, 9, 15, 21, 8, 23, 7, 9, 9, 8, 15, 21, 7, 9, 9, 21], [21, 1, 4, 5, 9, 9, 21, 8, 15, 2, 13, 23, 23, 7, 9, 9, 8, 15, 21, 7], [21, 1, 4, 5, 9, 9, 8, 15, 21, 23, 7, 9, 9, 8, 15, 21, 7, 9, 9, 8]]\n",
      "[[21, 1, 2, 13, 23, 4, 2, 13, 16, 1, 2, 13, 23, 2, 13, 23, 5, 16, 1, 16], [21, 1, 2, 4, 13, 23, 5, 9, 9, 15, 21, 8, 2, 13, 23, 23, 7, 9, 9, 8], [2, 13, 21, 23, 1, 4, 5, 9, 9, 15, 21, 8, 23, 10, 7, 9, 9, 8, 15, 21]]\n"
     ]
    }
   ],
   "source": [
    "pos = makeTrain(dic_train_pos_pos)\n",
    "neg = makeTrain(dic_train_neg_neg)\n",
    "\n",
    "print len(pos)\n",
    "print len(neg)\n",
    "\n",
    "print pos[0:3]\n",
    "print neg[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1015\n",
      "1015 1015\n",
      "13000 13000\n",
      "2030 2030\n",
      "(2030,)\n",
      "[ 1.  1.  1. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import copy\n",
    "\n",
    "random.shuffle(neg)\n",
    "\n",
    "tlen = 6500\n",
    "\n",
    "trainX = copy.deepcopy(pos[0:tlen])\n",
    "trainX.extend(neg[0:tlen])\n",
    "trainY = np.ones(tlen)\n",
    "trainY = np.append(trainY, np.zeros(tlen))\n",
    "\n",
    "testX = copy.deepcopy(pos[tlen:])\n",
    "leng = len(testX)\n",
    "testY = np.ones(leng)\n",
    "\n",
    "print leng\n",
    "print len(testX),len(testY)\n",
    "testX.extend(neg[tlen:tlen+leng])\n",
    "testY = np.append(testY, np.zeros(leng))\n",
    "\n",
    "print len(trainX),len(trainY)\n",
    "print len(testX),len(testY)\n",
    "print testY.shape\n",
    "\n",
    "print trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainX = np.array(trainX)\n",
    "testX = np.array(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 128)         3328      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 128)         131584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, None, 128)         512       \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 275,841.0\n",
      "Trainable params: 275,329\n",
      "Non-trainable params: 512.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(26, 128))\n",
    "model.add(LSTM(128, dropout=0.3, recurrent_dropout=0.2, return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(128, dropout=0.3, recurrent_dropout=0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(64,activation='tanh'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 13000 samples, validate on 2030 samples\n",
      "Epoch 1/20\n",
      "13000/13000 [==============================] - 75s - loss: 0.4460 - acc: 0.7745 - val_loss: 0.3864 - val_acc: 0.7951\n",
      "Epoch 2/20\n",
      "13000/13000 [==============================] - 71s - loss: 0.4012 - acc: 0.7862 - val_loss: 0.3797 - val_acc: 0.8030\n",
      "Epoch 3/20\n",
      "13000/13000 [==============================] - 70s - loss: 0.3961 - acc: 0.7858 - val_loss: 0.3738 - val_acc: 0.7956\n",
      "Epoch 4/20\n",
      "13000/13000 [==============================] - 69s - loss: 0.3884 - acc: 0.7912 - val_loss: 0.3816 - val_acc: 0.8108\n",
      "Epoch 5/20\n",
      "13000/13000 [==============================] - 70s - loss: 0.3867 - acc: 0.7984 - val_loss: 0.3620 - val_acc: 0.8158\n",
      "Epoch 6/20\n",
      "13000/13000 [==============================] - 70s - loss: 0.3809 - acc: 0.8025 - val_loss: 0.3727 - val_acc: 0.8153\n",
      "Epoch 7/20\n",
      "13000/13000 [==============================] - 68s - loss: 0.3774 - acc: 0.8028 - val_loss: 0.3648 - val_acc: 0.8108\n",
      "Epoch 8/20\n",
      "13000/13000 [==============================] - 68s - loss: 0.3753 - acc: 0.8052 - val_loss: 0.3667 - val_acc: 0.8128\n",
      "Epoch 9/20\n",
      "13000/13000 [==============================] - 68s - loss: 0.3758 - acc: 0.8028 - val_loss: 0.3662 - val_acc: 0.8064\n",
      "Epoch 10/20\n",
      "13000/13000 [==============================] - 76s - loss: 0.3722 - acc: 0.8042 - val_loss: 0.3625 - val_acc: 0.8163\n",
      "Epoch 11/20\n",
      "13000/13000 [==============================] - 75s - loss: 0.3694 - acc: 0.8097 - val_loss: 0.3648 - val_acc: 0.8207\n",
      "Epoch 12/20\n",
      "13000/13000 [==============================] - 72s - loss: 0.3695 - acc: 0.8072 - val_loss: 0.3582 - val_acc: 0.8182\n",
      "Epoch 13/20\n",
      "13000/13000 [==============================] - 68s - loss: 0.3661 - acc: 0.8127 - val_loss: 0.3570 - val_acc: 0.8232\n",
      "Epoch 14/20\n",
      "13000/13000 [==============================] - 68s - loss: 0.3673 - acc: 0.8120 - val_loss: 0.3606 - val_acc: 0.8192\n",
      "Epoch 15/20\n",
      "13000/13000 [==============================] - 68s - loss: 0.3636 - acc: 0.8149 - val_loss: 0.3551 - val_acc: 0.8207\n",
      "Epoch 16/20\n",
      "13000/13000 [==============================] - 68s - loss: 0.3635 - acc: 0.8114 - val_loss: 0.3560 - val_acc: 0.8153\n",
      "Epoch 17/20\n",
      "13000/13000 [==============================] - 69s - loss: 0.3589 - acc: 0.8158 - val_loss: 0.3605 - val_acc: 0.8163\n",
      "Epoch 18/20\n",
      "13000/13000 [==============================] - 68s - loss: 0.3606 - acc: 0.8154 - val_loss: 0.3573 - val_acc: 0.8197\n",
      "Epoch 19/20\n",
      "13000/13000 [==============================] - 70s - loss: 0.3611 - acc: 0.8142 - val_loss: 0.3521 - val_acc: 0.8227\n",
      "Epoch 20/20\n",
      "13000/13000 [==============================] - 69s - loss: 0.3545 - acc: 0.8177 - val_loss: 0.3535 - val_acc: 0.8212\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13415fe10>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(trainX, trainY,\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "          shuffle=True,\n",
    "          validation_data=(testX, testY))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1499849138.53\n",
      "1499849157.17\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "print time.time()\n",
    "result = model.predict(trainX)\n",
    "print time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13000\n",
      "5984\n",
      "2262\n",
      "5743\n",
      "1746\n"
     ]
    }
   ],
   "source": [
    "#验证训练数据\n",
    "print len(result)\n",
    "posl = result[:tlen]\n",
    "negl = result[tlen:]\n",
    "\n",
    "posloc = np.where(posl>=0.35)\n",
    "negloc = np.where(negl>=0.35)\n",
    "print len(posloc[0])\n",
    "print len(negloc[0])\n",
    "\n",
    "\n",
    "posloc = np.where(posl>=0.45)\n",
    "negloc = np.where(negl>=0.45)\n",
    "\n",
    "print len(posloc[0])\n",
    "print len(negloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.47538951]\n",
      " [ 0.51247615]\n",
      " [ 0.48711824]\n",
      " ..., \n",
      " [ 0.48644963]\n",
      " [ 0.12136541]\n",
      " [ 0.07643696]]\n"
     ]
    }
   ],
   "source": [
    "print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultpos = model.predict(np.array(pos))\n",
    "resultneg = model.predict(np.array(neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7515\n",
      "7129\n",
      "分到正类百分比： 0.948636061211\n",
      "12186\n",
      "5290\n",
      "分到负类百分比： 0.565895289677\n"
     ]
    }
   ],
   "source": [
    "rate=0.2\n",
    "\n",
    "posloc = np.where(resultpos>=rate)\n",
    "negloc = np.where(resultpos<=rate)\n",
    "print len(resultpos)\n",
    "print len(posloc[0])\n",
    "print \"分到正类百分比：\",len(posloc[0])*1.0/len(resultpos)\n",
    "\n",
    "posloc = np.where(resultneg>=rate)\n",
    "negloc = np.where(resultneg<=rate)\n",
    "print len(resultneg)\n",
    "print len(posloc[0])\n",
    "print \"分到负类百分比：\",len(negloc[0])*1.0/len(resultneg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 20)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)          (None, 20, 32)        800                                          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                    (None, 20, 128)       82432                                        \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                    (None, 32)            20608                                        \n",
      "____________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)          (None, 1, 32)         800                                          \n",
      "____________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)              (None, 32)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 32)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 64)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 64)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 512)           33280                                        \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 512)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 64)            32832                                        \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 64)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 1)             65                                           \n",
      "====================================================================================================\n",
      "Total params: 170,817.0\n",
      "Trainable params: 170,817.0\n",
      "Non-trainable params: 0.0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#推荐使用的模型\n",
    "from keras.layers import Input, Embedding, LSTM, Dense,Flatten, Dropout, merge\n",
    "from keras.layers import Input, Activation, Permute, Dropout, add, dot, concatenate\n",
    "from keras.layers.core import Reshape\n",
    "from keras.models import Model\n",
    "input_1=Input(shape=(20,), dtype='int32')\n",
    "input_2=Input(shape=(1,), dtype='int32')\n",
    "x1=Embedding(output_dim=32, input_dim=25, input_length=20)(input_1)\n",
    "x1=LSTM(128, return_sequences=True)(x1)\n",
    "x1=LSTM(32)(x1)\n",
    "x1=Reshape((-1,))(x1)\n",
    "x2=Embedding(output_dim=32, input_dim=25, input_length=1)(input_2)\n",
    "x2=Flatten()(x2)\n",
    "x = concatenate([x1, x2])\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "out = Dense(1, activation='sigmoid')(x)\n",
    "model = Model([input_1, input_2], out)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1. ...,  0.  0.  0.]\n",
      "[ 1.  1.  1. ...,  0.  0.  0.]\n",
      "\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "⋮ \n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.LongTensor of size 13000]\n",
      "\n",
      "Variable containing:\n",
      "   21     1     2  ...     23     9     2\n",
      "   21     1     5  ...     23     3     7\n",
      "   21     1     2  ...     13    23     8\n",
      "       ...          ⋱          ...       \n",
      "    1    21     4  ...      9     9    21\n",
      "   21     1     2  ...      9     8    15\n",
      "   21     1     4  ...      9     9     8\n",
      "[torch.LongTensor of size 1752x20]\n",
      "\n",
      "[ 1.  1.  1. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "#pytorch\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "from torch import nn\n",
    "torch.manual_seed(1)    # reproducible\n",
    "\n",
    "BATCH_SIZE = 32      # 批训练的数据个数\n",
    "\n",
    "\n",
    "torchx = torch.from_numpy(trainX)\n",
    "#torchx = Variable(torchx)\n",
    "print trainY\n",
    "#trainY.dtype = 'int64'\n",
    "torchy = torch.from_numpy(trainY).type(torch.LongTensor)\n",
    "print trainY\n",
    "print torchy\n",
    "torch_test_x = Variable(torch.from_numpy(testX), volatile=True) \n",
    "print torch_test_x\n",
    "torch_test_y = testY.squeeze()    # covert to numpy array\n",
    "print torch_test_y\n",
    "\n",
    "#torchy = Variable(torchy)\n",
    "\n",
    "# 先转换成 torch 能识别的 Dataset\n",
    "torch_dataset = Data.TensorDataset(data_tensor=torchx, target_tensor=torchy)\n",
    "\n",
    "# 把 dataset 放入 DataLoader\n",
    "loader = Data.DataLoader(\n",
    "    dataset=torch_dataset,      # torch TensorDataset format\n",
    "    batch_size=BATCH_SIZE,      # mini batch size\n",
    "    shuffle=True,               # 要不要打乱数据 (打乱比较好)\n",
    "    num_workers=2,              # 多线程来读数据\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net (\n",
      "  (embed): Embedding(26, 128)\n",
      "  (rnn): LSTM(128, 128, num_layers=2, batch_first=True)\n",
      "  (hidden): Linear (128 -> 64)\n",
      "  (predict): Linear (64 -> 2)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(torch.nn.Module):  # 继承 torch 的 Module\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()     # 继承 __init__ 功能\n",
    "        # 定义每层用什么样的形式\n",
    "        self.embed = torch.nn.Embedding(26, 128)\n",
    "        self.rnn = nn.LSTM(     # LSTM 效果要比 nn.RNN() 好多了\n",
    "            input_size=128,      # \n",
    "            hidden_size=128,     # rnn hidden unit\n",
    "            num_layers=2,       # 有几层 RNN layers\n",
    "            batch_first=True,   # input & output 会是以 batch size 为第一维度的特征集 e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "        self.hidden = torch.nn.Linear(128, 64)   # 隐藏层线性输出\n",
    "        self.predict = torch.nn.Linear(64, 2)   # 输出层线性输出\n",
    "\n",
    "    def forward(self, x):   # 这同时也是 Module 中的 forward 功能\n",
    "        x = self.embed(x)\n",
    "        r_out, (h_n, h_c) = self.rnn(x, None) \n",
    "        #x = F.relu(self.hidden(x))      # 激励函数(隐藏层的线性值)\n",
    "        x = F.relu(self.hidden(r_out[:, -1, :]))\n",
    "        x = self.predict(x)\n",
    "        return x\n",
    "    \n",
    "rnn = Net()\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# optimizer 是训练的工具\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=0.5)  # 传入 net 的所有参数, 学习率\n",
    "#optimizer = torch.optim.Adam(rnn.parameters(), lr=0.1) \n",
    "\n",
    "# 算误差的时候, 注意真实值!不是! one-hot 形式的, 而是1D Tensor, (batch,)\n",
    "# 但是预测值是2D tensor (batch, n_classes)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for t in range(1):\n",
    "    for step, (batch_x, batch_y) in enumerate(loader):\n",
    "        pass\n",
    "        #print type(batch_x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1499682397.8\n",
      "[0 1 1 ..., 0 0 0]\n",
      "('Epoch: ', 0, '| train loss: 0.3129', '| test accuracy: 0.81')\n",
      "[0 1 1 ..., 0 0 0]\n",
      "('Epoch: ', 1, '| train loss: 0.2392', '| test accuracy: 0.80')\n",
      "[0 1 1 ..., 0 0 0]\n",
      "('Epoch: ', 2, '| train loss: 0.4402', '| test accuracy: 0.82')\n",
      "[0 1 1 ..., 0 0 0]\n",
      "('Epoch: ', 3, '| train loss: 0.2290', '| test accuracy: 0.81')\n",
      "[0 1 1 ..., 0 0 0]\n",
      "('Epoch: ', 4, '| train loss: 0.2646', '| test accuracy: 0.83')\n",
      "[0 1 1 ..., 0 0 0]\n",
      "('Epoch: ', 5, '| train loss: 0.1874', '| test accuracy: 0.82')\n",
      "[0 1 1 ..., 0 0 0]\n",
      "('Epoch: ', 6, '| train loss: 0.3087', '| test accuracy: 0.83')\n",
      "[0 1 1 ..., 0 0 0]\n",
      "('Epoch: ', 7, '| train loss: 0.3967', '| test accuracy: 0.83')\n",
      "[0 1 1 ..., 0 0 0]\n",
      "('Epoch: ', 8, '| train loss: 0.4457', '| test accuracy: 0.82')\n",
      "[0 1 1 ..., 0 0 0]\n",
      "('Epoch: ', 9, '| train loss: 0.3595', '| test accuracy: 0.81')\n",
      "[0 1 1 ..., 0 0 0]\n",
      "('Epoch: ', 10, '| train loss: 0.1810', '| test accuracy: 0.82')\n",
      "[0 1 1 ..., 0 0 0]\n",
      "('Epoch: ', 11, '| train loss: 0.1153', '| test accuracy: 0.81')\n",
      "[0 1 1 ..., 0 0 0]\n",
      "('Epoch: ', 12, '| train loss: 0.3036', '| test accuracy: 0.81')\n",
      "[0 1 1 ..., 0 0 0]\n",
      "('Epoch: ', 13, '| train loss: 0.1226', '| test accuracy: 0.82')\n",
      "[0 1 1 ..., 0 0 0]\n",
      "('Epoch: ', 14, '| train loss: 0.4940', '| test accuracy: 0.83')\n",
      "[0 1 1 ..., 0 0 0]\n",
      "('Epoch: ', 15, '| train loss: 0.0618', '| test accuracy: 0.82')\n",
      "[0 1 1 ..., 0 0 0]\n",
      "('Epoch: ', 16, '| train loss: 0.5635', '| test accuracy: 0.83')\n",
      "[0 1 1 ..., 0 0 0]\n",
      "('Epoch: ', 17, '| train loss: 0.5503', '| test accuracy: 0.83')\n",
      "[0 1 1 ..., 0 0 0]\n",
      "('Epoch: ', 18, '| train loss: 0.3011', '| test accuracy: 0.82')\n",
      "[0 1 1 ..., 0 0 0]\n",
      "('Epoch: ', 19, '| train loss: 0.3001', '| test accuracy: 0.83')\n",
      "1499683045.27\n"
     ]
    }
   ],
   "source": [
    "print time.time()\n",
    "for t in range(20):\n",
    "    for step, (batch_x, batch_y) in enumerate(loader):\n",
    "        b_x = Variable(batch_x)              # reshape x to (batch, time_step, input_size)\n",
    "        b_y = Variable(batch_y)                               # batch y\n",
    "        #print b_x\n",
    "        \n",
    "        output = rnn(b_x)         # rnn output\n",
    "        loss = loss_func(output, b_y)                   # cross entropy loss\n",
    "        optimizer.zero_grad()                           # clear gradients for this training step\n",
    "        loss.backward()                                 # backpropagation, compute gradients\n",
    "        optimizer.step() \n",
    "        \n",
    "        \n",
    "    test_output = rnn(torch_test_x)\n",
    "    #test_output = rnn(b_x)\n",
    "    \n",
    "    #print test_output\n",
    "    pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "    print pred_y\n",
    "    accuracy = sum(pred_y == torch_test_y) / float(torch_test_y.size)\n",
    "    print('Epoch: ', t, '| train loss: %.4f' % loss.data[0], '| test accuracy: %.2f' % accuracy)\n",
    "print time.time()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_output = rnn(b_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 6.8061 -6.6220\n",
      " 7.2712 -6.5124\n",
      " 6.8020 -6.4809\n",
      " 6.8020 -6.4809\n",
      " 6.8489 -6.5292\n",
      " 6.6334 -6.6031\n",
      " 6.8020 -6.4809\n",
      " 7.1402 -6.7817\n",
      "[torch.FloatTensor of size 8x2]\n",
      "\n",
      "Variable containing:\n",
      " 0.0000e+00\n",
      " 4.6072e+18\n",
      " 0.0000e+00\n",
      " 0.0000e+00\n",
      " 4.6072e+18\n",
      " 4.6072e+18\n",
      " 0.0000e+00\n",
      " 0.0000e+00\n",
      "[torch.LongTensor of size 8]\n",
      "\n",
      "\n",
      " 0.0000e+00\n",
      " 4.6072e+18\n",
      " 0.0000e+00\n",
      " 0.0000e+00\n",
      " 4.6072e+18\n",
      " 4.6072e+18\n",
      " 0.0000e+00\n",
      " 0.0000e+00\n",
      "[torch.LongTensor of size 8]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print test_output\n",
    "print b_y\n",
    "print batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
