{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Activation\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import sys\n",
    "import jieba\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#max_features = 20000\n",
    "#maxlen = 80\n",
    "batch_size = 20\n",
    "learning_rate = 0.01\n",
    "file_path = './dataset/data/周杰伦歌词大全_cleaned.txt'\n",
    "checkpoints_dir = './checkpoints/lyrics'\n",
    "model_prefix = 'lyrics'\n",
    "epochs = 200\n",
    "start_token = 'G'\n",
    "end_token = 'E'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_lyrics(file_path):\n",
    "    lyrics = []\n",
    "    #content = clean_cn_corpus(file_name, clean_level='all', is_save=False)\n",
    "    fr = open(file_path)\n",
    "    \n",
    "    for l in fr.readlines():\n",
    "        if len(l) < 80:\n",
    "            continue\n",
    "        l = start_token + l.strip() + end_token\n",
    "        lyrics.append(l)\n",
    "    lyrics = sorted(lyrics, key=lambda line: len(line))\n",
    "    #print(lyrics)\n",
    "    print('all %d songs...' % len(lyrics))\n",
    "\n",
    "    all_words = []\n",
    "    for lyric in lyrics:\n",
    "        all_words += jieba.lcut(lyric, cut_all=False)\n",
    "\n",
    "    # calculate how many time appear per word\n",
    "    counter = collections.Counter(all_words)\n",
    "    print(counter['E'])\n",
    "    # sorted depends on frequent\n",
    "    counter_pairs = sorted(counter.items(), key=lambda x: -x[1])\n",
    "    words, _ = zip(*counter_pairs)\n",
    "    #print(words)\n",
    "    print('E' in words)\n",
    "\n",
    "    #words = words[:len(words)] + (' ',)\n",
    "    word_int_map = dict(zip(words, range(len(words))))\n",
    "    lyrics_vector = []\n",
    "    # translate all lyrics into int vector\n",
    "    for lyric in lyrics:\n",
    "        word = jieba.lcut(lyric, cut_all=False)\n",
    "        lyrics_vector.append([word_int_map[w] for w in word])\n",
    "        \n",
    "    #lyrics_vector = [list(map(lambda word: word_int_map.get(word, len(words)), lyric)) for lyric in lyrics]\n",
    "    #print(lyrics_vector)\n",
    "    return lyrics_vector, word_int_map, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch(batch_size, lyrics_vec, word_to_int):\n",
    "    # split all lyrics into n_chunks * batch_size\n",
    "    n_chunk = len(lyrics_vec) // batch_size\n",
    "    \n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    for i in range(n_chunk):\n",
    "        start_index = i * batch_size\n",
    "        end_index = start_index + batch_size\n",
    "\n",
    "        batches = lyrics_vec[start_index:end_index]\n",
    "        # very batches length depends on the longest lyric\n",
    "        length = max(map(len, batches))\n",
    "        # 填充一个这么大小的空batch，空的地方放空格对应的index标号\n",
    "        x_data = np.full((batch_size, length), word_to_int[' '], np.int32)\n",
    "        for row in range(batch_size):\n",
    "            x_data[row, :len(batches[row])] = batches[row]\n",
    "        y_data = np.copy(x_data)\n",
    "        # y的话就是x向左边也就是前面移动一个\n",
    "        y_data[:, :-1] = x_data[:, 1:]\n",
    "        \"\"\"\n",
    "        x_data             y_data\n",
    "        [6,2,4,6,9]       [2,4,6,9,9]\n",
    "        [1,4,2,8,5]       [4,2,8,5,5]\n",
    "        \"\"\"\n",
    "        x_batches.append(x_data)\n",
    "        y_batches.append(y_data)\n",
    "    return x_batches, y_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_data(lyrics_vec, word_to_int):\n",
    "    # split all lyrics into n_chunks * batch_size\n",
    "    n_cnt = len(lyrics_vec)\n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    for i in range(n_cnt):\n",
    "        batches = lyrics_vec[i]\n",
    "        # very batches length depends on the longest lyric\n",
    "        length = len(batches)\n",
    "        # 填充一个这么大小的空batch，空的地方放空格对应的index标号\n",
    "        x_data = np.full((length), word_to_int[' '], np.int32)\n",
    "        x_data[:len(batches)] = batches\n",
    "        y_data = np.copy(x_data)\n",
    "        # y的话就是x向左边也就是前面移动一个\n",
    "        y_data[:-1] = x_data[1:]\n",
    "        \"\"\"\n",
    "        x_data             y_data\n",
    "        [6,2,4,6,9]       [2,4,6,9,9]\n",
    "        [1,4,2,8,5]       [4,2,8,5,5]\n",
    "        \"\"\"\n",
    "        x_batches.append(x_data)\n",
    "        y_batches.append(y_data)\n",
    "    return x_batches, y_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all 131 songs...\n",
      "131\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(os.path.dirname(checkpoints_dir)):\n",
    "    os.mkdir(os.path.dirname(checkpoints_dir))\n",
    "if not os.path.exists(checkpoints_dir):\n",
    "    os.mkdir(checkpoints_dir)\n",
    "\n",
    "poems_vector, word_to_int, vocabularies = process_lyrics(file_path)\n",
    "vocab_size = len(vocabularies)\n",
    "#batches_inputs, batches_outputs = generate_batch(batch_size, poems_vector, word_to_int)\n",
    "bi,bo = generate_data(poems_vector, word_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlen = max([len(b) for b in bi])\n",
    "bis=sequence.pad_sequences(bi, padding='post',maxlen=mlen,value=word_to_int[end_token])\n",
    "bos=sequence.pad_sequences(bo, padding='post',maxlen=mlen,value=word_to_int[end_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, None, 128)         733056    \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "repeat_vector_8 (RepeatVecto (None, 481, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, 481, 128)          131584    \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 481, 5726)         738654    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 481, 5726)         0         \n",
      "=================================================================\n",
      "Total params: 1,734,878.0\n",
      "Trainable params: 1,734,878.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size+1, 128))\n",
    "#model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2,return_sequences=True))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(RepeatVector(mlen))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(vocab_size)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "yc=to_categorical(bos,vocab_size)\n",
    "yd=yc.reshape(bos.shape[0],bos.shape[1],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Epoch 1/1\n",
      "131/131 [==============================] - 152s - loss: 8.5600 - acc: 0.5070    \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 2\n",
      "Epoch 1/1\n",
      "131/131 [==============================] - 152s - loss: 7.3094 - acc: 0.6093    \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 3\n",
      "Epoch 1/1\n",
      "131/131 [==============================] - 139s - loss: 5.6637 - acc: 0.6093    \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 4\n",
      "Epoch 1/1\n",
      "131/131 [==============================] - 148s - loss: 4.3504 - acc: 0.6093    \n"
     ]
    }
   ],
   "source": [
    "for iteration in range(1, 5):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(bis,yd, batch_size=batch_size, epochs=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14]]\n",
      "(481, 5726)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([list(map(word_to_int.get, start_token))])\n",
    "print(x)\n",
    "y=model.predict(x)\n",
    "print(y[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
